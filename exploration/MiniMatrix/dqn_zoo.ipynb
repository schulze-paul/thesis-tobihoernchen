{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import time\n",
                "import json\n",
                "from alpyne.client.alpyne_client import AlpyneClient\n",
                "from stable_baselines3 import DQN\n",
                "from stable_baselines3.common.utils import set_random_seed\n",
                "from stable_baselines3.common.env_util import make_vec_env\n",
                "\n",
                "sys.path.append(\"../..\")\n",
                "from thesis.envs.matrix_routing_zoo_cyclic import getMatrixRoutingMACyclic\n",
                "from thesis.policies.routing_attention import RoutingFE_offPolicy\n",
                "from thesis.policies.ppo_ac_attention import AttentionACPolicy\n",
                "seed = 42\n",
                "set_random_seed(seed)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "models_dir = \"../../models/MiniMatrix_Routing_MA\"\n",
                "logdir = \"../../logs/MiniMatrix_Routing_MA\"\n",
                "fleetsize = 2\n",
                "max_fleetsize = 3\n",
                "run_name = f\"DQNZoo-{fleetsize}-{max_fleetsize}-{time.strftime('%d_%m-%H_%M_%S')}-{seed}\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": [
                "env_args = dict(\n",
                "        reward_target = 1, \n",
                "        reward_distance = 0.05,\n",
                "        reward_block = -0.5, \n",
                "        dispatchinginterval=60,\n",
                "        routinginterval = 2,\n",
                "        withCollisions = True,\n",
                "        blockTimeout = 5,\n",
                "        includeNodesInReach = True,\n",
                "    )\n",
                "\n",
                "dqn_args = dict(\n",
                "    #learning_rate = 3e-3,\n",
                "    #buffer_size = 100000\n",
                ")\n",
                "fe_args = dict(\n",
                "    max_fleetsize=max_fleetsize,\n",
                "    embed_dim = 64,\n",
                "    n_heads = 8,\n",
                "    depth = 8\n",
                ")\n",
                "net_arch = dict(qf = [], pi = [])\n",
                "\n",
                "hparams = dict(\n",
                "    fleetsize = fleetsize,\n",
                "    max_fleetsize = max_fleetsize,\n",
                "    env_args = env_args,\n",
                "    ppo_args = env_args,\n",
                "    fe_args = fe_args,\n",
                "    net_arch = net_arch\n",
                ")\n",
                "with open(f\"{models_dir}/{run_name}.json\", 'w') as outfile:\n",
                "    json.dump(hparams, outfile, indent = 3)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\alpyne\\client\\utils.py:124: UserWarning: Unzipping to temporary directory (C:\\Users\\Wegma\\AppData\\Local\\Temp\\alpyne_74836_4_njj_06)\n",
                        "  warn(f\"Unzipping to temporary directory ({tmp_dir})\")\n"
                    ]
                }
            ],
            "source": [
                "i = [0]\n",
                "\n",
                "client = AlpyneClient(\"../../envs/MiniMatrix.zip\", port=51151)\n",
                "\n",
                "env = getMatrixRoutingMACyclic(\n",
                "    #verbose = True,\n",
                "    max_seconds = 60*60, \n",
                "    fleetsize = fleetsize, \n",
                "    max_fleetsize=max_fleetsize, \n",
                "    config_args = env_args,\n",
                "    counter = i,\n",
                "    client = client,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "model =DQN(\n",
                "    \"MlpPolicy\",\n",
                "    env, \n",
                "    tensorboard_log= logdir,\n",
                "    device = \"cuda\",\n",
                "    policy_kwargs=dict(\n",
                "        #net_arch = net_arch,\n",
                "        features_extractor_class=RoutingFE_offPolicy, \n",
                "        features_extractor_kwargs=fe_args\n",
                "        ),\n",
                "    optimize_memory_usage=True,\n",
                "    **dqn_args,\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "\u001b[1;32md:\\Master\\Masterarbeit\\thesis\\exploration\\MiniMatrix\\dqn_zoo.ipynb Zelle 6\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Master/Masterarbeit/thesis/exploration/MiniMatrix/dqn_zoo.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m TIMESTEPS \u001b[39m=\u001b[39m \u001b[39m100000\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Master/Masterarbeit/thesis/exploration/MiniMatrix/dqn_zoo.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m15\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Master/Masterarbeit/thesis/exploration/MiniMatrix/dqn_zoo.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49mTIMESTEPS, reset_num_timesteps\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, tb_log_name\u001b[39m=\u001b[39;49mrun_name)\n",
                        "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:264\u001b[0m, in \u001b[0;36mDQN.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    252\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    253\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    261\u001b[0m     reset_num_timesteps: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    262\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OffPolicyAlgorithm:\n\u001b[1;32m--> 264\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    265\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    266\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    267\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    268\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[0;32m    269\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[0;32m    270\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[0;32m    271\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    272\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[0;32m    273\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    274\u001b[0m     )\n",
                        "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:363\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[39m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[0;32m    362\u001b[0m         \u001b[39mif\u001b[39;00m gradient_steps \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 363\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, gradient_steps\u001b[39m=\u001b[39;49mgradient_steps)\n\u001b[0;32m    365\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[0;32m    367\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
                        "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py:199\u001b[0m, in \u001b[0;36mDQN.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    196\u001b[0m     target_q_values \u001b[39m=\u001b[39m replay_data\u001b[39m.\u001b[39mrewards \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m replay_data\u001b[39m.\u001b[39mdones) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m next_q_values\n\u001b[0;32m    198\u001b[0m \u001b[39m# Get current Q-values estimates\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m current_q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_net(replay_data\u001b[39m.\u001b[39;49mobservations)\n\u001b[0;32m    201\u001b[0m \u001b[39m# Retrieve the q-values for the actions from the replay buffer\u001b[39;00m\n\u001b[0;32m    202\u001b[0m current_q_values \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mgather(current_q_values, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, index\u001b[39m=\u001b[39mreplay_data\u001b[39m.\u001b[39mactions\u001b[39m.\u001b[39mlong())\n",
                        "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
                        "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\stable_baselines3\\dqn\\policies.py:66\u001b[0m, in \u001b[0;36mQNetwork.forward\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, obs: th\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m th\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m     60\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39m    Predict the q-values.\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \n\u001b[0;32m     63\u001b[0m \u001b[39m    :param obs: Observation\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39m    :return: The estimated Q-Value for each action.\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_net(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract_features(obs))\n",
                        "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\stable_baselines3\\common\\policies.py:129\u001b[0m, in \u001b[0;36mBaseModel.extract_features\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures_extractor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mNo features extractor was set\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    128\u001b[0m preprocessed_obs \u001b[39m=\u001b[39m preprocess_obs(obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space, normalize_images\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnormalize_images)\n\u001b[1;32m--> 129\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures_extractor(preprocessed_obs)\n",
                        "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
                        "File \u001b[1;32md:\\Master\\Masterarbeit\\thesis\\exploration\\MiniMatrix\\../..\\thesis\\policies\\routing_attention.py:125\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    123\u001b[0m ablocks \u001b[39m=\u001b[39m []\n\u001b[0;32m    124\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(depth):\n\u001b[1;32m--> 125\u001b[0m     ablocks\u001b[39m.\u001b[39mappend(\n\u001b[0;32m    126\u001b[0m         AttentionBlock(\n\u001b[0;32m    127\u001b[0m             embed_dim\u001b[39m=\u001b[39membed_dim, n_heads\u001b[39m=\u001b[39mn_heads\n\u001b[0;32m    128\u001b[0m         )  \u001b[39m# , fn_mask=get_mask)\u001b[39;00m\n\u001b[0;32m    129\u001b[0m     )\n\u001b[0;32m    130\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mablocks \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\u001b[39m*\u001b[39mablocks)\n",
                        "File \u001b[1;32md:\\Master\\Masterarbeit\\thesis\\exploration\\MiniMatrix\\../..\\thesis\\policies\\routing_attention.py:105\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(observation_space, spaces\u001b[39m.\u001b[39mDict):\n\u001b[0;32m    102\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_obs_per_agv \u001b[39m=\u001b[39m (\n\u001b[0;32m    103\u001b[0m         observation_space\u001b[39m.\u001b[39mspaces[\u001b[39m\"\u001b[39m\u001b[39mobservation\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m max_fleetsize\n\u001b[0;32m    104\u001b[0m     )\n\u001b[1;32m--> 105\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_fleetsize \u001b[39m=\u001b[39m max_fleetsize\n\u001b[0;32m    106\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maye \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
                        "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
                        "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
                        "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
                        "File \u001b[1;32md:\\Master\\Masterarbeit\\thesis\\exploration\\MiniMatrix\\../..\\thesis\\policies\\routing_attention.py:43\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m         fedforward = self.ff(x)\n\u001b[0;32m     42\u001b[0m         x = self.norm2(fedforward + x)\n\u001b[1;32m---> 43\u001b[0m         # x = x * self.fn_mask().detach()\n\u001b[0;32m     44\u001b[0m         return x\n\u001b[0;32m     47\u001b[0m class Nodewise_Embedder(nn.Module):\n",
                        "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
                        "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
                        "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
                        "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\torch\\nn\\modules\\activation.py:98\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> 98\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
                        "File \u001b[1;32mc:\\Users\\Wegma\\.conda\\envs\\thesis\\lib\\site-packages\\torch\\nn\\functional.py:1442\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1440\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[0;32m   1441\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1442\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m   1443\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "TIMESTEPS = 100000\n",
                "for i in range(1, 15):\n",
                "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=run_name)#,callback=MiniMatrixCallback())\n",
                "    #model.save(f\"{models_dir}/{run_name}-{TIMESTEPS * i}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8.13 ('thesis')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.13"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "dc7f4105f9f5e395f215a7643dd52717d50b308583dcde27027fbaaaba0d8cea"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
